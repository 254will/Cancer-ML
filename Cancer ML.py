# -*- coding: utf-8 -*-
"""Cancer_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1563EUlU5JxYVaT11zQyxN6RF-phwRhFg

# **Preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from scipy import stats

from google.colab import files
import io
import pandas as pd


uploaded = files.upload()
for file_name in uploaded.keys():
    raw_data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=';')

    print(f"Data from file: {file_name}")
    print(raw_data.head())

data = pd.read_csv('/content/Raw Data.csv', delimiter=';')

data.head()

"""**Encoding Features**

1. County
"""

sorted(data['county'].unique())

county = pd.get_dummies(data['county'])
county.head()

county['check'] = county.sum(axis=1)
print(county['check'].sum(axis=0))
print(county['check'].unique())
county.drop(['check'], axis=1, inplace=True)

data.drop(['county'], axis=1, inplace=True)

county['Coast Province'] = county.loc[:, 1:6].any(axis=1)
county['Northern Kenya'] = county.loc[:, 9:10].any(axis=1)
county['Eastern Province'] = county.loc[:, 11:17].any(axis=1)
county['Central Province'] = county.loc[:, 18:22].any(axis=1)
county['Rift Valley Province'] = county.loc[:, 25:36].any(axis=1)
county['Western Province'] = county.loc[:, 37:40].any(axis=1)
county['Nyanza Kenya'] = county.loc[:, 41:46].any(axis=1)
county['Nairobi Province'] = county.loc[:, 47:].any(axis=1)

data = pd.concat([data, county['Coast Province'], county['Northern Kenya'], county['Eastern Province'], county['Central Province'], county['Rift Valley Province'], county['Western Province'], county['Nyanza Kenya'], county['Nairobi Province']], axis=1)
data['Nairobi & Central Province'] = data[['Nairobi Province', 'Central Province']].any(axis=1)
data.drop(['Nairobi Province', 'Central Province'], axis=1, inplace=True)
data.head()

data.columns.values

column_names_reordered = ['dob', 'sex', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Nairobi & Central Province',
       'alcohol', 'drug', 'cigarette', 'Diagnoses',
       'cancerstage', 'hivstatus', 'child', 'housing', 'insurance',
       'transport', 'work', 'treatment', 'dealingwithchildren',
       'dealingwithpartner', 'abilitytohavechild', 'familyhealthissues',
       'depression', 'fear', 'nervous', 'sadness', 'worry',
       'lossofinterest', 'spiritual', 'apperance', 'bathing', 'breathing',
       'changesurination', 'constipation', 'diarrhea', 'eating',
       'fatigue', 'feelingswollen', 'fever', 'gettingaround',
       'indigestion', 'memory', 'sores', 'nausea', 'nosedry', 'pain',
       'sexual', 'skindry', 'sleep', 'substance', 'tinglinghands',
       'discharge', 'vision', 'speech', 'bleeding', 'dizziness',
       'hearing', 'vomiting', 'numbness', 'bweakness', 'dhydration',
       'appetite', 'wloss', 'cough']

data = data[column_names_reordered]
data.head()

"""2. Alcohol, Drugs $ Cigarette"""

alcohol = pd.get_dummies(data['alcohol'])
drug = pd.get_dummies(data['drug'])
cigarette = pd.get_dummies(data['cigarette'])
print(alcohol.head())
print(drug.head())
print(cigarette.head())

alcohol['check'] = alcohol.sum(axis=1)
print(alcohol['check'].sum(axis=0))
print(alcohol['check'].unique())
alcohol.drop(['check'], axis=1, inplace=True)
alcohol.drop([4], axis=1, inplace=True)

drug['check'] = drug.sum(axis=1)
print(drug['check'].sum(axis=0))
print(drug['check'].unique())
drug.drop(['check'], axis=1, inplace=True)
drug.drop([4], axis=1, inplace=True)

cigarette['check'] = cigarette.sum(axis=1)
print(cigarette['check'].sum(axis=0))
print(cigarette['check'].unique())
cigarette.drop([4], axis=1, inplace=True)

data.drop(['alcohol'], axis=1, inplace=True)
data.drop(['drug'], axis=1, inplace=True)
data.drop(['cigarette'], axis=1, inplace=True)

alcohol['Alcohol_Use'] = alcohol.loc[:, 1:2].any(axis=1)
alcohol['Alcohol_NonUse'] = alcohol.loc[:, 3:].any(axis=1)

drug['Drug_Use'] = drug.loc[:, 1:2].any(axis=1)
drug['Drug_NonUse'] = drug.loc[:, 3:].any(axis=1)

cigarette['Cigarette_Use'] = cigarette.loc[:, 1:2].any(axis=1)
cigarette['Cigarette_NonUse'] = cigarette.loc[:, 3:].any(axis=1)

data = pd.concat([data, alcohol['Alcohol_Use'], alcohol['Alcohol_NonUse'], drug['Drug_Use'], drug['Drug_NonUse'], cigarette['Cigarette_Use'], cigarette['Cigarette_NonUse']], axis=1)
data.head()

data['Alcohol, Drug & Cigarette Use'] = data[['Alcohol_Use', 'Drug_Use', 'Cigarette_Use']].any(axis=1)

data = data.drop(columns=['Alcohol_Use', 'Drug_Use', 'Cigarette_Use', 'Alcohol_NonUse', 'Drug_NonUse', 'Cigarette_NonUse'])

data.columns.values

column_names_reordered = ['dob', 'sex', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Nairobi & Central Province', 'Alcohol, Drug & Cigarette Use','Diagnoses',
       'cancerstage', 'hivstatus', 'child', 'housing', 'insurance',
       'transport', 'work', 'treatment', 'dealingwithchildren',
       'dealingwithpartner', 'abilitytohavechild', 'familyhealthissues',
       'depression', 'fear', 'nervous', 'sadness', 'worry',
       'lossofinterest', 'spiritual', 'apperance', 'bathing', 'breathing',
       'changesurination', 'constipation', 'diarrhea', 'eating',
       'fatigue', 'feelingswollen', 'fever', 'gettingaround',
       'indigestion', 'memory', 'sores', 'nausea', 'nosedry', 'pain',
       'sexual', 'skindry', 'sleep', 'substance', 'tinglinghands',
       'discharge', 'vision', 'speech', 'bleeding', 'dizziness',
       'hearing', 'vomiting', 'numbness', 'bweakness', 'dhydration',
       'appetite', 'wloss', 'cough']

data = data[column_names_reordered]
data.columns.values

data.head()

"""3. Practical Problems"""

from sklearn.preprocessing import LabelEncoder

data['Practical Problems'] = data[['child', 'housing', 'insurance', 'transport', 'work', 'treatment']].astype(str).agg(''.join, axis=1)

label_encoder = LabelEncoder()

data['Practical Problems'] = label_encoder.fit_transform(data['Practical Problems'])

data.drop(['child', 'housing', 'insurance', 'transport', 'work', 'treatment'], axis=1, inplace=True)
data.columns.values

"""4. Family Problems"""

data['Family Problems'] = data[['dealingwithchildren','dealingwithpartner', 'abilitytohavechild', 'familyhealthissues']].astype(str).agg(''.join, axis=1)

label_encoder = LabelEncoder()

data['Family Problems'] = label_encoder.fit_transform(data['Family Problems'])

data.drop(['dealingwithchildren','dealingwithpartner', 'abilitytohavechild', 'familyhealthissues'], axis=1, inplace=True)
data.columns.values

"""5. Emotional Problems"""

data['Emotional Problems'] = data[['depression', 'fear', 'nervous', 'sadness', 'worry', 'lossofinterest', 'spiritual']].astype(str).agg(''.join, axis=1)

label_encoder = LabelEncoder()

data['Emotional Problems'] = label_encoder.fit_transform(data['Emotional Problems'])

data.drop(['depression', 'fear', 'nervous', 'sadness', 'worry', 'lossofinterest', 'spiritual'], axis=1, inplace=True)
data.columns.values

"""6. Physical Problems"""

data['Physical Problems'] = data[['apperance', 'bathing', 'breathing',
       'changesurination', 'constipation', 'diarrhea', 'eating',
       'fatigue', 'feelingswollen', 'fever', 'gettingaround',
       'indigestion', 'memory', 'sores', 'nausea', 'nosedry', 'pain',
       'sexual', 'skindry', 'sleep', 'substance', 'tinglinghands',
       'discharge', 'vision', 'speech', 'bleeding', 'dizziness',
       'hearing', 'vomiting', 'numbness', 'bweakness', 'dhydration',
       'appetite', 'wloss', 'cough']].astype(str).agg(''.join, axis=1)

label_encoder = LabelEncoder()

data['Physical Problems'] = label_encoder.fit_transform(data['Physical Problems'])

data.drop(['apperance', 'bathing', 'breathing',
       'changesurination', 'constipation', 'diarrhea', 'eating',
       'fatigue', 'feelingswollen', 'fever', 'gettingaround',
       'indigestion', 'memory', 'sores', 'nausea', 'nosedry', 'pain',
       'sexual', 'skindry', 'sleep', 'substance', 'tinglinghands',
       'discharge', 'vision', 'speech', 'bleeding', 'dizziness',
       'hearing', 'vomiting', 'numbness', 'bweakness', 'dhydration',
       'appetite', 'wloss', 'cough'], axis=1, inplace=True)
data.columns.values

data.head()

"""7. Diagnosis"""

categories = {
    'Head & Neck Cancers': [9, 12, 17, 18, 21, 23, 32, 33, 34, 36, 37],
    'Reproductive Cancers': [13, 19, 38, 41, 42, 43, 45, 46, 47, 48],
    'Digestive System Cancers': [1, 2, 3, 4, 5, 6, 11, 14, 15, 16, 20, 22, 24, 25, 27, 29, 30, 43, 45, 46],
    'Blood & Bone Cancers': [7, 8, 58, 59],
    'Skin & Soft Tissue Cancers': [47, 48, 50],
    'Miscellaneous Cancers': [35, 33, 49, 52, 53, 54, 55, 56, 57, 60]
}

for category, numbers in categories.items():
    data[category] = data['Diagnoses'].apply(lambda x: 1 if x in numbers else 0)

data = data.drop(columns=['Diagnoses'])
data.columns.values

data.head()

"""8. Cancer Stages"""

categories = {
    'Early Cancer Stages': [1,2,3],
    'Late Cancer Stages': [4,5],
    'Not Staged': [6]
}

for category, numbers in categories.items():
    data[category] = data['cancerstage'].apply(lambda x: 1 if x in numbers else 0)

data = data.drop(columns=['cancerstage'])
data.columns.values

"""9. HIV Status"""

categories = {
    'Positive HIV Status': [1],
    'Negative HIV Status': [2],
    'Unknown HIV Status': [3]
}

for category, numbers in categories.items():
    data[category] = data['hivstatus'].apply(lambda x: 1 if x in numbers else 0)

data = data.drop(columns=['hivstatus'])
data.columns.values

column_names_reordered = ['dob', 'sex', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Nairobi & Central Province',
       'Alcohol, Drug & Cigarette Use',
       'Early Cancer Stages', 'Late Cancer Stages', 'Not Staged',
       'Positive HIV Status', 'Negative HIV Status', 'Unknown HIV Status',
       'Practical Problems', 'Family Problems', 'Emotional Problems', 'Physical Problems',
       'Head & Neck Cancers', 'Reproductive Cancers',
       'Digestive System Cancers', 'Blood & Bone Cancers',
       'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers']

data = data[column_names_reordered]
data.columns.values

column_names = ['Age', 'Gender', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Nairobi & Central Province',
       'Alcohol, Drug & Cigarette Use', 'Early Cancer Stages',
       'Late Cancer Stages', 'Not Staged', 'Positive HIV Status',
       'Negative HIV Status', 'Unknown HIV Status', 'Practical Problems',
       'Family Problems', 'Emotional Problems', 'Physical Problems',
       'Head & Neck Cancers', 'Reproductive Cancers',
       'Digestive System Cancers', 'Blood & Bone Cancers',
       'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers']

# Assigning new names to 'dob' and 'sex'
data.columns = column_names

print(data.dtypes)

"""Checking for null features"""

# Check which features have null values
null_columns = data.columns[data.isnull().any()]
print(f"Features with null values:\n{null_columns}")

# Replace null values with the mean for numeric features
for col in null_columns:
    if data[col].dtype in ['int64', 'float64']:  # Ensure that only numeric columns are processed
        mean_value = data[col].mean()
        data[col].fillna(mean_value, inplace=True)
        print(f"Replaced null values in '{col}' with mean: {mean_value}")
    else:
        print(f"Skipping '{col}' because it's not numeric.")

# Check if the null values have been handled
print("Updated dataset with null values replaced by mean:")
print(data.isnull().sum())

"""Scaling Problem List & Age"""

from sklearn.preprocessing import StandardScaler

# Features to scale
features_to_scale = ['Practical Problems', 'Family Problems', 'Emotional Problems', 'Physical Problems', 'Age']

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the specified features
data[features_to_scale] = scaler.fit_transform(data[features_to_scale])

# Check the first few rows of the scaled data
print(data.head())

# Convert boolean columns to integers
bool_columns = data.select_dtypes(include=[bool]).columns
data[bool_columns] = data[bool_columns].astype(int)

print(data.dtypes)

"""Checking for multicollinearity"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# Create a new dataframe with the independent variables
X = data[['Age', 'Gender', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Nairobi & Central Province',
       'Alcohol, Drug & Cigarette Use', 'Early Cancer Stages',
       'Late Cancer Stages', 'Not Staged', 'Positive HIV Status',
       'Negative HIV Status', 'Unknown HIV Status', 'Practical Problems',
       'Family Problems', 'Emotional Problems', 'Physical Problems']]

# Adding a constant column (intercept)
X = sm.add_constant(X)

# Define a function to calculate VIFs
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data['Feature'] = X.columns
    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

# Set a threshold for VIF (5 or 10)
vif_threshold = 5

# Iteratively remove features with high VIF
while True:
    # Calculate the VIF values
    vif_data = calculate_vif(X)
    print(vif_data)

    # Check if all VIF values are below the threshold
    if vif_data['VIF'].max() > vif_threshold:
        # Identify the feature with the highest VIF
        feature_to_remove = vif_data.sort_values('VIF', ascending=False).iloc[0]['Feature']
        print(f"Removing feature with the highest VIF: {feature_to_remove}")

        # Drop the feature with the highest VIF from the dataframe
        X = X.drop(columns=[feature_to_remove])
    else:
        # If all VIF values are below the threshold, break the loop
        print("All VIF values are below the threshold.")
        break

# Final VIF values
print("Final VIF values:")
print(calculate_vif(X))

# Check if 'const' exists before dropping
if 'const' in X.columns:
    preprocessed_data = X.drop(columns=['const'])
else:
    preprocessed_data = X

# Add the target variables (cancer types) back into the preprocessed data
target_vars = data[['Head & Neck Cancers', 'Reproductive Cancers', 'Digestive System Cancers',
                    'Blood & Bone Cancers', 'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers']]
final_data = pd.concat([preprocessed_data, target_vars], axis=1)

final_data.columns.values

"""Export Data"""

from google.colab import files

final_data.to_csv('Preprocessed Data.csv', index=False)

files.download('Preprocessed Data.csv')

"""# **Logit Regression Model**"""

import numpy as np
import statsmodels.api as sm
import pandas as pd

from google.colab import files
import io
uploaded = files.upload()

for file_name in uploaded.keys():
    preprocessed_data = pd.read_csv(io.BytesIO(uploaded[file_name]))

preprocessed_data = pd.read_csv('/content/Preprocessed Data.csv', delimiter=',')
preprocessed_data.head()

import numpy as np
import statsmodels.api as sm
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Assuming `preprocessed_data` contains your independent (X) and dependent variables (y)

# Define the independent variables (X)
X = preprocessed_data.drop(columns=['Head & Neck Cancers', 'Reproductive Cancers',
                                    'Digestive System Cancers', 'Blood & Bone Cancers',
                                    'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers'])

# Add a constant to the model (intercept)
X = sm.add_constant(X)

# Define a function to calculate VIFs
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data['Feature'] = X.columns
    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

# Calculate VIF and remove highly collinear features
vif_threshold = 5  # Set a threshold for VIF (5 is commonly used)
while True:
    vif_data = calculate_vif(X)
    print(vif_data)

    # If the maximum VIF is above the threshold, remove the feature with the highest VIF
    if vif_data['VIF'].max() > vif_threshold:
        feature_to_remove = vif_data.sort_values('VIF', ascending=False).iloc[0]['Feature']
        print(f"Removing feature due to high VIF: {feature_to_remove}")
        X = X.drop(columns=[feature_to_remove])
    else:
        print("All VIF values are below the threshold. Proceeding with the model.")
        break

# Define the dependent variables (y)
y_vars = ['Head & Neck Cancers', 'Reproductive Cancers', 'Digestive System Cancers',
          'Blood & Bone Cancers', 'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers']

# Dictionary to store results
results = {}

# Fit a logistic regression model for each dependent variable and display coefficients and odds ratios
for y_var in y_vars:
    y = preprocessed_data[y_var]  # Current dependent variable

    # Fit the logistic regression model
    model = sm.Logit(y, X)

    try:
        result = model.fit()
        # Store the result in the dictionary
        results[y_var] = result

        # Get the coefficients and odds ratios
        coefficients = result.params
        odds_ratios = np.exp(result.params)

        # Display results
        print(f"Logistic Regression for {y_var}")
        print(f"Coefficients:\n{coefficients}\n")
        print(f"Odds Ratios:\n{odds_ratios}\n")
        print(f"Model Summary:\n{result.summary()}\n")
    except np.linalg.LinAlgError as e:
        print(f"Failed to fit model for {y_var} due to singular matrix: {e}")

preprocessed_data.columns.values

"""Dropping Insignificant Features"""

Significant_data = preprocessed_data.drop(columns=['Gender', 'Coast Province', 'Northern Kenya',
       'Eastern Province', 'Rift Valley Province', 'Western Province',
       'Nyanza Kenya', 'Alcohol, Drug & Cigarette Use', 'Practical Problems',
       'Family Problems', 'Emotional Problems', 'Physical Problems'])
Significant_data.columns.values

from google.colab import files

Significant_data.to_csv('Significant Data.csv', index=False)
files.download('Significant Data.csv')

"""# **Machine Learning**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import preprocessing

"""1. Load and Prepare Data"""

from google.colab import files
import io
import numpy as np

uploaded = files.upload()

for file_name in uploaded.keys():
    data = np.genfromtxt(io.BytesIO(uploaded[file_name]), delimiter=',', skip_header=1)

significant_data = np.genfromtxt('/content/Significant Data.csv', delimiter=',', skip_header=1)
data = significant_data.copy()

data

X = data[:, 1:7]
y = data[:, -6:]

print(X.shape)
print(y.shape )

"""2. ML Algorithm"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
import tensorflow as tf

# Assuming `X` and `y` are numpy arrays with inputs and multi-label outputs
y_df = pd.DataFrame(y, columns=['Head & Neck Cancers', 'Reproductive Cancers',
                                'Digestive System Cancers', 'Blood & Bone Cancers',
                                'Skin & Soft Tissue Cancers', 'Miscellaneous Cancers'])

# Step 1: Combine X and y into a single dataframe to resample them together
data = np.hstack((X, y_df.values))  # Combine features (X) and labels (y) horizontally

# Step 2: Separate positive and negative cases based on any of the labels (e.g., Head & Neck Cancers)
positive_cases = data[data[:, -6:].any(axis=1) == 1]  # Rows where at least one label is positive
negative_cases = data[data[:, -6:].all(axis=1) == 0]  # Rows where all labels are negative

# Step 3: Resample (oversample) positive cases to match the number of negative cases
positive_upsampled = resample(positive_cases,
                              replace=True,
                              n_samples=len(negative_cases),
                              random_state=42)

# Step 4: Combine the resampled data back together
resampled_data = np.vstack((negative_cases, positive_upsampled))

# Step 5: Separate the resampled X and y
X_resampled = resampled_data[:, :-6]  # All but the last 6 columns are features (X)
y_resampled = resampled_data[:, -6:]  # The last 6 columns are the labels (y)

# Step 6: Split the resampled data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 7: Normalize the Input Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 8: Define the Neural Network Model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # First hidden layer
    tf.keras.layers.Dropout(0.3),  # Dropout for regularization
    tf.keras.layers.Dense(16, activation='relu'),  # Second hidden layer
    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')  # Output layer for multi-label classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.Precision(),
                       tf.keras.metrics.Recall()])

# Step 9: Train the Model
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train_scaled, y_train,
                    validation_data=(X_test_scaled, y_test),
                    epochs=50,
                    batch_size=32,
                    callbacks=[early_stopping])

# Step 10: Evaluate the Model
test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test_scaled, y_test)
print(f'Test Accuracy: {test_acc}')
print(f'Test Precision: {test_precision}')
print(f'Test Recall: {test_recall}')

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Threshold the predictions
y_pred_binary = (y_pred > 0.3).astype(int)

# Calculate F1 score for each label and overall (macro or micro averaging)
from sklearn.metrics import f1_score, hamming_loss
f1 = f1_score(y_test, y_pred_binary, average='macro')  # 'macro' or 'micro'
print(f'F1 Score: {f1}')

# Calculate Hamming Loss
h_loss = hamming_loss(y_test, y_pred_binary)
print(f'Hamming Loss: {h_loss}')

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

# Make predictions on the test set
predictions = model.predict(X_test)

# Threshold predictions (e.g., treat outputs > 0.5 as 1, otherwise 0)
predicted_labels = (predictions > 0.5).astype(int)